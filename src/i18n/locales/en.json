{
  "navigation": {
    "home": "Home",
    "hami": "HAMi",
    "whatIsHami": "What is HAMi",
    "whatIsHamiDesc": "Learn about the core features and value of the HAMi open source project",
    "hamiWebsite": "HAMi Website",
    "hamiWebsiteDesc": "Visit the official HAMi website for detailed information",
    "hamiGithub": "GitHub",
    "hamiGithubDesc": "View the source code and contribute to HAMi's open source collaboration",
    "community": "Community",
    "communityDesc": "Join the HAMi user community to share experiences and get help",
    "products": "Products",
    "caseStudies": "Case Studies",
    "solutions": "Solutions",
    "pricing": "Pricing",
    "resources": "Resources",
    "resourcesDoc": "Documentation",
    "resourcesDocDesc": "Access comprehensive guides and reference materials for HAMi",
    "resourcesBlog": "Blog",
    "resourcesBlogDesc": "Read the latest news, tutorials and updates about HAMi",
    "adopters": "Adopters",
    "adoptersDesc": "See who's using HAMi in production environments",
    "company": "Company",
    "freeTrial": "Apply for Trial",
    "requestDemo": "Request Demo",
    "openMenu": "Open menu",
    "caseTelecom": "Telecom Provider Case",
    "caseTelecomDesc": "Learn how a telecom provider leverages HAMi for GPU virtualization and heterogeneous device management",
    "caseKeHoldings": "Ke Holdings Case",
    "caseKeHoldingsDesc": "Discover how Ke Holdings scales ML infrastructure with GPU virtualization using Kubernetes and HAMi",
    "caseDaoCloud": "DaoCloud Case",
    "caseDaoCloudDesc": "Discover how DaoCloud builds flexible GPU clouds with HAMi across D.run Compute Cloud and DaoCloud Enterprise",
    "caseSfTechnology": "SF Technology Case",
    "caseSfTechnologyDesc": "Discover how SF Technology builds EffectiveGPU based on HAMi to enhance AI efficiency and reduce costs",
    "casePrepEdu": "PREP EDU Case",
    "casePrepEduDesc": "Explore how PREP EDU achieves intelligent scheduling of Heterogeneous GPUs based on HAMi to boost resource utilization and O&M efficiency"
  },
  "home": {
    "hero": {
      "title": "Open source, Accelerate AI Workflows With GPU sharing, Dynamic Orchestration",
      "subtitle": "Dynamic AI GPU sharing, scheduling and orchestration that accelerates AI throughput, delivers seamless auto scaling, and maximizes GPU utilization.",
      "chineseSlogan": "Unified heterogeneous computing for resource efficiency"
    },
    "socialProof": {
      "title": "Trusted by leading AI labs and enterprises worldwide"
    },
    "keyAdvantages": {
      "title": "Why us?",
      "advantages": [
        {
          "title": "High-Performance Scheduling",
          "description": "Optimize resource allocation with intelligent workload scheduling"
        },
        {
          "title": "Multi-Cluster Orchestration",
          "description": "Seamlessly manage resources across multiple clusters and environments"
        },
        {
          "title": "GPU Acceleration Management",
          "description": "Maximize the use of specialized hardware for AI, ML, and HPC workloads"
        },
        {
          "title": "Cost Optimization & Autoscaling",
          "description": "Automatically scale resources up or down based on workload demands"
        }
      ],
      "tabs": [
        {
          "title": "Hetero multi-cluster",
          "subtitle": "Unified AI Infrastructure Management",
          "description": "Dynamia.ai provides a centralized approach to managing AI infrastructure, ensuring optimal workload distribution across hybrid, multi-cloud, and on-premises environments."
        },
        {
          "title": "GPU Sharing",
          "subtitle": "Maximize GPU Utilization, Minimize Costs, and Drive AI Efficiency",
          "description": "Dynamia.ai dynamically consolidates and orchestrates GPU resources. By eliminating waste, maximizing resource utilization, enterprises achieve superior ROI, reduced operational costs, and faster scaling of AI initiatives."
        },
        {
          "title": "GPU Oversubscription",
          "subtitle": "Alignment of GPU and host memory to expand the available resource pool",
          "description": "Dynamia.ai supports seamlessly unify GPU and host memory to maximize the efficiency of co-located AI workloads."
        },
        {
          "title": "Seamlessly Auto Scale",
          "subtitle": "Seamlessly Accelerate AI From V1 to V1 Plus",
          "description": "Dynamia.ai  support GPU on-demand auto-scaling, seamless auto-scaling for AI workloads without restarts during GPU consumption surges."
        }
      ]
    },
    "poweredByHami": {
      "title": "Powered by HAMi",
      "description": "Leading, open source, heterogeneous AI computing virtualization middleware",
      "learnMore": "Learn More",
      "stats": {
        "contributors": "+60",
        "forks": "+270",
        "stars": "+2.8K",
        "commits": "+600",
        "pulls": "+50K"
      }
    },
    "joinCommunity": {
      "title": "Join our community",
      "bilibili": {
        "title": "Follow us on Bilibili",
        "action": "Follow"
      },
      "slack": {
        "title": "Talk with us on Slack",
        "action": "Join"
      },
      "discord": {
        "title": "Chat with us on Discord",
        "action": "Join"
      },
      "twitter": {
        "title": "Follow us on X",
        "action": "Follow"
      },
      "reddit": {
        "title": "Discuss on Reddit",
        "action": "Join"
      },
      "wechat": {
        "title": "WeChat",
        "action": "Scan QR Code"
      }
    },
    "ecosystem": {
      "title": "Ecosystem Integration",
      "subtitle": "GPU virtualization and scheduling integration powered by HAMi",
      "viewDetails": "View details"
    },
    "testimonials": {
      "title": "What Our Customers Say",
      "items": [
        {
          "quote": "Kantaloupe has dramatically improved our computational efficiency, allowing us to scale our AI research faster than ever before.",
          "author": "Dr. Jane Smith",
          "position": "CTO",
          "company": "AI Research Institute"
        },
        {
          "quote": "The seamless integration across our heterogeneous computing environment has reduced deployment time by 60%.",
          "author": "Michael Johnson",
          "position": "VP of Engineering",
          "company": "TechCorp Global"
        }
      ]
    },
    "cta": {
      "title": "Ready to transform your computing infrastructure?",
      "freeTrialButton": "Apply for Trial",
      "requestDemoButton": "Request a Demo"
    },
    "trustedBy": {
      "title": "Trusted by Industry Leaders",
      "description": "Leading organizations across various industries rely on Dynamia.ai to manage their heterogeneous computing resources efficiently."
    }
  },
  "products": {
    "kantaloupe": {
      "title": "Dynamia AI",
      "subtitle": "Enterprise-grade heterogeneous computing virtualization platform",
      "viewPricing": "View Pricing",
      "readyToStart": "Ready to Transform Your Computing Infrastructure?",
      "ctaDescription": "Experience how Dynamia AI can optimize your computing resources. Get started today with a trial or request a custom pricing quote.",
      "overview": {
        "title": "Enterprise-Ready Heterogeneous Computing Solution",
        "description": "Dynamia AI builds on the powerful foundation of HAMi open-source technology, adding enterprise-grade features, security, and support for mission-critical environments.",
        "highlights": [
          "Built on HAMi open-source core",
          "Enhanced enterprise features",
          "Professional technical support",
          "Continuous updates and maintenance"
        ]
      },
      "featureComparison": {
        "title": "Feature Comparison",
        "subTitle": "HAMi vs Dynamia AI Feature Comparison",
        "categoryHeader": "",
        "featureHeader": "",
        "openSource": "HAMi",
        "enterprise": "Dynamia AI",
        "categories": [
          {
            "name": "Architecture Support",
            "features": [
              {
                "name": "x86",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "ARM",
                "hamiStatus": true,
                "kantaloupeStatus": true
              }
            ]
          },
          {
            "name": "Operating Systems",
            "features": [
              {
                "name": "Linux",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "WSL",
                "hamiStatus": false,
                "kantaloupeStatus": true
              }
            ]
          },
          {
            "name": "Scale",
            "features": [
              {
                "name": "Nodes",
                "hamiStatus": "1 - 20 nodes",
                "kantaloupeStatus": "20 - 1000 nodes"
              },
              {
                "name": "GPUs",
                "hamiStatus": "1 - 160 GPUs",
                "kantaloupeStatus": "100 - 8000 GPUs"
              }
            ]
          },
          {
            "name": "Core Capabilities",
            "features": [
              {
                "name": "GPU Memory share",
                "hamiStatus": "✅",
                "kantaloupeStatus": true
              },
              {
                "name": "QOS GPU Memory share",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "GPU Core share",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "QOS GPU Core share",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "GPU Memory oversubscribe",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "Computing Core oversubscribe",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "Dynamic Adjustment",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "Auto Scaling",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "Task Priority",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Fault Isolation",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Task Idle Time Statistics",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "Memory Analysis",
                "hamiStatus": false,
                "kantaloupeStatus": true
              }
            ]
          },
          {
            "name": "Scheduling Capabilities",
            "features": [
              {
                "name": "Node-level Binpack, Spread Scheduling",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "GPU-level Binpack, Spread Scheduling",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Nvidia MIG",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Nvidia MIG Dynamic Virtualization",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Nvidia NUMA Affinity Scheduling",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Nvidia Network Topology Affinity Scheduling",
                "hamiStatus": false,
                "kantaloupeStatus": true
              }
            ]
          },
          {
            "name": "Heterogeneous Capabilities",
            "features": [
              {
                "name": "Cambricon MLU",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Hygon DCU",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Ascend 310 Series",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Ascend 910 Series",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Moore Threads GPU",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Iluvatar GPU",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Enflame GPU",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Metax GPU",
                "hamiStatus": true,
                "kantaloupeStatus": true
              }
            ]
          },
          {
            "name": "Monitoring Capabilities",
            "features": [
              {
                "name": "GPU Allocation of AI Workloads",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "Node Binding AI workloads",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "AI workloads usage",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "GPU Fault Metrics",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "GPU Fault Alerts",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "GPU Fault Self-recovery",
                "hamiStatus": false,
                "kantaloupeStatus": true
              }
            ]
          },
          {
            "name": "AI Multi-cluster Management",
            "features": [
              {
                "name": "AI Cluster Lifecycle Management",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "AI Multi-cluster Unified Management View",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "AI Multi-cluster Unified Monitoring and Operations View",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "Multi-cluster Workbench UI",
                "hamiStatus": false,
                "kantaloupeStatus": true
              }
            ]
          },
          {
            "name": "Operations Capabilities",
            "features": [
              {
                "name": "GPU Stack LCM",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "GUI",
                "hamiStatus": true,
                "kantaloupeStatus": true
              },
              {
                "name": "OpenAPI",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "Metering and Billing",
                "hamiStatus": false,
                "kantaloupeStatus": true
              }
            ]
          },
          {
            "name": "Security",
            "features": [
              {
                "name": "KataContainer",
                "hamiStatus": false,
                "kantaloupeStatus": true
              },
              {
                "name": "Security Policy",
                "hamiStatus": false,
                "kantaloupeStatus": true
              }
            ]
          },
          {
            "name": "Support",
            "features": [
              {
                "name": "Support",
                "hamiStatus": "✅ Community support: GitHub, Slack channel",
                "kantaloupeStatus": "✅ Professional team support"
              }
            ]
          }
        ]
      },
      "technicalArchitecture": "Technical Architecture",
      "architectureDescription": "Dynamia AI provides a comprehensive infrastructure for managing heterogeneous computing resources efficiently.",
      "architectureDiagram": "Architecture Diagram",
      "commercialFeatures": {
        "title": "Enterprise Features",
        "subtitle": "Dynamia AI builds on HAMi's core capabilities to deliver enterprise-grade enhancements optimized for production environments.",
        "list": [
          {
            "title": "Hetero multi-cluster",
            "description": "Dynamia.ai provides a centralized approach to managing AI infrastructure, ensuring optimal workload distribution across hybrid, multi-cloud, and on-premises environments."
          },
          {
            "title": "GPU Sharing",
            "description": "Dynamia.ai dynamically consolidates and orchestrates GPU resources. By eliminating waste, maximizing resource utilization, enterprises achieve superior ROI, reduced operational costs, and faster scaling of AI initiatives."
          },
          {
            "title": "GPU Oversubscription",
            "description": "Dynamia.ai supports seamlessly unify GPU and host memory to maximize the efficiency of co-located AI workloads."
          },
          {
            "title": "Seamlessly Auto Scale",
            "description": "GPU on-demand auto-scaling, seamless VPA scaling for AI workloads without restarts during GPU consumption surges."
          },
          {
            "title": "Centralized Observability for Complete AI insight",
            "description": "Its centralized observability unifies resources from cloud, on-premises, and hybrid environments, empowering enterprises with actionable insights, policy-driven governance, and fine-grained resource management for efficient and scalable AI operations. "
          },
          {
            "title": "Advanced AI Scheduling",
            "description": "Advanced AI Scheduling for different scenarios including numa-aware, binpack, spread. Reduce data communication cost, minimize fragments, optimize task performance."
          }
        ]
      }
    }
  },
  "solutions": {
    "title": "Solutions",
    "subtitle": "Powerful computing solutions for complex challenges",
    "categories": [
      {
        "title": "AI/ML",
        "description": "Optimize your machine learning infrastructure for faster training and inference",
        "learnMore": "Learn more"
      },
      {
        "title": "HPC",
        "description": "Scale your high-performance computing workloads efficiently",
        "learnMore": "Learn more"
      },
      {
        "title": "Edge Computing",
        "description": "Extend your computing capabilities to the edge with optimized resource allocation",
        "learnMore": "Learn more"
      }
    ],
    "customSolutions": {
      "title": "Custom Solutions",
      "description": "We understand that every organization has unique needs. Our expert team can customize Dynamia AI solutions for your specific challenges.",
      "contactButton": "Contact our solution experts"
    },
    "successStories": {
      "title": "Success Stories",
      "subtitle": "Learn how our solutions have helped customers address complex computing challenges",
      "viewAllButton": "View all stories"
    },
    "readyForChallenge": "Ready to address your computing challenges?"
  },
  "freeTrial": {
    "title": "Apply for a Trial",
    "subtitle": "Start your trial and experience the power of Dynamia AI",
    "form": {
      "name": "Name",
      "email": "Work Email",
      "company": "Company Name",
      "phone": "Phone Number",
      "useCase": "How do you plan to use Dynamia AI?",
      "terms": "I agree to dynamia ai's Terms of Service and Privacy Policy",
      "termsRequired": "You must agree to the terms and conditions to proceed.",
      "submitting": "Processing...",
      "submitButton": "Apply for Trial",
      "submitSuccess": "Thank you! Your trial application has been submitted. Our team will contact you shortly with next steps.",
      "submitError": "An error occurred while submitting your request. Please try again or contact us directly at info@dynamia.ai."
    },
    "disclaimer": "After your trial ends, you can choose to upgrade to a paid plan or cancel your account. No credit card required."
  },
  "requestDemo": {
    "title": "Request a Demo",
    "subtitle": "Schedule a personalized demo to learn how Dynamia AI can help your organization optimize computing resources",
    "demoContent": {
      "title": "Demo Content",
      "items": [
        "Overview of Dynamia AI platform features",
        "Customized demonstration tailored to your needs",
        "Interactive session with our solution experts",
        "Detailed exploration of deployment options and pricing"
      ]
    },
    "form": {
      "title": "Your Information",
      "name": "Name",
      "email": "Work Email",
      "company": "Company Name",
      "jobTitle": "Job Title",
      "message": "What would you like to learn in the demo?",
      "submitting": "Sending request...",
      "submitButton": "Request Demo",
      "submitSuccess": "Thank you! Your demo request has been received. Our team will contact you within 24 hours to schedule your personalized demo.",
      "submitError": "An error occurred while submitting your request. Please try again or contact us directly at info@dynamia.ai."
    },
    "followUp": "After submitting your request, our team will contact you within 24 hours to schedule your demo."
  },
  "resources": {
    "title": "Resources",
    "subtitle": "Explore our technical documentation, blog articles, and whitepapers to learn more about Dynamia AI",
    "viewMore": "View more",
    "documentation": {
      "title": "Documentation",
      "description": "Technical guides, API references, and best practices"
    },
    "blog": {
      "title": "Blog",
      "description": "Insights, tutorials, and news from our team",
      "tableOfContents": "On this page",
      "aiShare": {
        "exploreTitle": "Explore with AI",
        "exploreSubtitle": "Use AI tools to gain deeper understanding",
        "shareTitle": "Share this article",
        "shareAt": "Share at:",
        "tipLabel": "Tip",
        "tip": "AI will help you summarize key points and analyze technical details.",
        "aiPrompt": "Visit this URL and summarize the post for me: {{url}}",
        "aiTools": {
          "chatgpt": {
            "name": "ChatGPT",
            "desc": "Smart Summary"
          },
          "perplexity": {
            "name": "Perplexity",
            "desc": "Deep Analysis"
          },
          "claude": {
            "name": "Claude",
            "desc": "Professional Insights"
          },
          "gemini": {
            "name": "Google AI",
            "desc": "Quick Insights"
          },
          "grok": {
            "name": "Grok",
            "desc": "X AI Analysis"
          },
          "deepseek": {
            "name": "DeepSeek",
            "desc": "Deep Analysis"
          }
        }
      }
    },
    "whitepapers": {
      "title": "Whitepapers",
      "description": "In-depth analysis and research reports"
    },
    "latestResources": {
      "title": "Latest Resources",
      "articles": [
        {
          "title": "Kantaloupe v2.0 Release: New Features and Improvements",
          "category": "Blog",
          "date": "2025-01-15",
          "excerpt": "Learn about the important updates and performance enhancements in the latest version of Kantaloupe.",
          "link": "/resources/blog/kantaloupe-v2-release"
        },
        {
          "title": "Heterogeneous Computing in Large-Scale AI Training",
          "category": "Whitepaper",
          "date": "2024-12-10",
          "excerpt": "Explore how to use Kantaloupe to optimize resource allocation for large-scale AI model training.",
          "link": "/resources/whitepapers/heterogeneous-computing-ai-training"
        },
        {
          "title": "Kantaloupe API Reference Guide",
          "category": "Documentation",
          "date": "2024-11-28",
          "excerpt": "Comprehensive API documentation to help developers integrate and extend the Kantaloupe platform.",
          "link": "/resources/documentation/api-reference"
        },
        {
          "title": "Case Study: How TechCorp Saved 40% on Computing Costs",
          "category": "Blog",
          "date": "2024-11-15",
          "excerpt": "Learn how TechCorp used Kantaloupe to optimize their HPC workloads and significantly reduce costs.",
          "link": "/resources/blog/techcorp-case-study"
        }
      ],
      "newsletterTitle": "Subscribe to Our Technical Newsletter",
      "newsletterDescription": "Receive regular updates on product improvements, technical articles, and industry insights delivered directly to your inbox.",
      "emailPlaceholder": "Your email address",
      "subscribeButton": "Subscribe"
    }
  },
  "company": {
    "about": {
      "title": "Who we are",
      "description": "We are pioneering the future of heterogeneous computing to help organizations maximize their computational resources",
      "whoWeAre": {
        "title": "Who we are",
        "paragraph1": "Dynamia.ai is the creator and maintainer of HAMi, which is the world's fastest-growing and most widely adopted open-source, heterogeneous AI computing virtualization middleware, scales AI workloads seamlessly across hybrid and multi-cloud environments.",
        "paragraph2": "Dynamia.ai offers an enterprise platform for AI workloads and GPU orchestration, unlocking the full potential of AI infrastructure.  This allows enterprises to effortlessly tackle the challenges of AI infrastructure utilization and hybrid complexity.",
        "paragraph3":""
      },
      "mission": {
        "title": "Our Mission",
        "description1": "At dynamia ai, we are committed to helping organizations maximize the value of their computing resources through innovative heterogeneous computing solutions. We believe that by intelligently managing and optimizing computing resources, we can accelerate scientific discovery, drive technological innovation, and create value for customers across industries worldwide.",
        "description2": "Our flagship product, Dynamia AI, is a unified heterogeneous computing platform designed to handle today's most complex computational challenges. Whether for AI training, high-performance computing, or edge computing, kantaloupe delivers unparalleled resource efficiency and performance optimization."
      },
      "vision": {
        "title": "Company Vision",
        "points": [
          "Build the world's leading heterogeneous computing management platform",
          "Drive efficient utilization of computing resources, reducing energy waste",
          "Empower organizations to achieve innovation through optimized computing infrastructure"
        ]
      }
    },
    "team": {
      "title": "Leadership",
      "members": [
        { "name": "Xiao Zhang", "position": "CEO & Founder & HAMi Maintainer", "image": "CEO" },
        { "name": "Mengxuan Li", "position": "Chief Architect & Co-founder & HAMi Maintainer", "image": "CTO" },
        { "name": "Yu Yin", "position": "Product Leader & HAMi Maintainer", "image": "COO" },
        { "name": "Wen Chen", "position": "Tech Leader", "image": "CPO" },
        { "name": "Jimmy Song", "position": "VP of Open Source Ecosystem", "image": "jimmy" },
        { "name": "Reza Jelveh", "position": "Head of Global Marketing", "image": "reza" }
      ]
    },
    "careers": {
      "title": "Careers",
      "description": "Join our team of innovators and shape the future of computing",
      "jobs": [
        { "title": "Senior Software Engineer", "location": "Beijing", "type": "Full-time" },
        { "title": "Machine Learning Engineer", "location": "Shanghai", "type": "Full-time" },
        { "title": "Product Manager", "location": "Remote", "type": "Full-time" },
        { "title": "Solutions Architect", "location": "Shenzhen", "type": "Full-time" }
      ],
      "viewJobsButton": "View all positions",
      "applyButton": "Apply"
    },
    "contact": {
      "title": "Contact",
      "description": "Get in touch with our team",
      "address": "123 Tech Avenue, San Francisco, CA 94105",
      "email": "info@dynamia.ai",
      "phone": "+1 (555) 123-4567",
      "formTitle": "Send a Message",
      "nameLabel": "Your Name",
      "emailLabel": "Your Email",
      "messageLabel": "Message",
      "submitting": "Sending...",
      "submitButton": "Send",
      "submitSuccess": "Thank you for your message! We will get back to you as soon as possible.",
      "submitError": "An error occurred while sending your message. Please try again or email us directly at info@dynamia.ai.",
      "social": {
        "email": {
          "title": "Email",
          "action": "Send Email"
        },
        "slack": {
          "title": "Slack",
          "action": "Join Us"
        },
        "twitter": {
          "title": "X (Twitter)",
          "action": "Follow Us"
        },
        "wechat": {
          "title": "WeChat",
          "action": "Scan QR Code",
          "qrPlaceholder": "WeChat QR Code Placeholder"
        }
      }
    }
  },
  "footer": {
    "copyright": "© 2025 DYNAMIA INTELLIGENCE PTE. LTD ",
    "privacyPolicy": "Privacy Policy",
    "cookiesPolicy": "Cookies Policy",
    "termsOfService": "Terms of Service",
    "company": "Company",
    "aboutUs": "About us",
    "contactUs": "Contact us",
    "platformTitle": "Dynamia AI Platform",
    "getDemo": "Get a demo",
    "pricing": "Pricing",
    "poweredByHami": "Powered by HAMi",
    "companyDescription": "Dynamia focuses on GPU virtualization and scheduling, leading the CNCF open-source project HAMi.",
    "companyIntro": "Enterprise-grade GPU virtualization and scheduling platform, optimized from the CNCF open-source project HAMi.",
    "about": "About",
    "aboutCompany": "Company Overview",
    "news": "News",
    "partners": "Partners",
    "joinUs": "Join Us",
    "resources": "Resources",
    "documentation": "Documentation",
    "blog": "Blog",
    "caseStudies": "Case Studies",
    "community": "Community",
    "github": "GitHub",
    "slack": "Slack",
    "discord": "Discord",
    "wechat": "WeChat",
    "contactSection": "Contact Us",
    "emailUs": "Email Us",
    "email": "info@dynamia.ai",
    "socialMedia": "Follow Us"
  },
  "search": {
    "placeholder": "Search...",
    "noResults": "No results found"
  },
  "hamiPage": {
    "title": "What is",
    "subtitle": "An open-source, heterogeneous AI computing virtualization middleware, scales AI workloads seamlessly across hybrid and multi-cloud environments.",
    "introduction": "HAMi is the only open-source project dedicated to GPU resource sharing. It maximizing GPU efficiency by flexible, reliable, and elastic GPU share, integrating seamlessly into hybrid AI infrastructure. Adopted by 60+ OS vendors, cloud providers, and industry leaders, it has enabled breakthroughs in finance, logistics, autonomous driving, robotics, and biotech.",
    "creator": "HAMi was created by dynamia.ai and is a Cloud Native Computing Foundation (CNCF) sandbox project.",
    "solutionTitle": "One-Stop Heterogeneous AI Computing Management Solution",
    "solutionDesc": "HAMi is a one-stop middleware that enables the sharing of various AI devices while ensuring resource isolation between different tasks. By improving the utilization of heterogeneous computing devices, HAMi provides a unified reuse interface to meet the needs of different device types.",
    "features": [
      {
        "title": "Kubernetes Native API Compatibility",
        "description": "One of HAMi's standout features is its compatibility with Kubernetes native APIs. This means users can upgrade to HAMi without modifying existing configurations, allowing for a seamless transition while maintaining Kubernetes default behaviors."
      },
      {
        "title": "Open and Neutral",
        "description": "HAMi is a collaborative initiative involving stakeholders from various domains, including internet services, finance, manufacturing, and cloud service providers. The goal is to establish open governance under the Cloud Native Computing Foundation (CNCF), ensuring HAMi remains neutral and accessible to all users."
      },
      {
        "title": "Avoid Vendor Lock-in",
        "description": "With HAMi, users can integrate with mainstream cloud service providers without being tied to proprietary vendor orchestrations. This flexibility allows organizations to choose their preferred cloud solution while leveraging HAMi's capabilities."
      },
      {
        "title": "Resource Isolation",
        "description": "HAMi provides robust resource isolation within containers. Each task running in a container is confined to its allocated resource boundaries, preventing any task from exceeding its quota. This strict isolation enhances security and stability in the computing environment."
      },
      {
        "title": "Support for Multiple Heterogeneous Computing Devices",
        "description": "HAMi excels in supporting various heterogeneous computing devices. Whether it's GPUs, MLUs, or NPUs from different manufacturers, HAMi facilitates device sharing and maximizes resource efficiency across different hardware platforms."
      },
      {
        "title": "Unified Management",
        "description": "To simplify operations, HAMi offers a unified monitoring system, along with configurable scheduling policies such as binpacking and spreading. This comprehensive management approach streamlines the governance of resources and enhances overall system performance."
      }
    ],
    "ctaTitle": "Ready to start using HAMi?",
    "ctaDesc": "Join our community and enhance your AI computing resource management efficiency.",
    "githubButton": "GitHub Repository",
    "websiteButton": "Visit Website"
  },
  "pricing": {
    "title": "Pricing",
    "headline": "Flexible Pricing for your Computational Needs",
    "description": "We offer a range of flexible pricing options tailored to the size and complexity of your computing environment. Contact us for a customized quote.",
    "benefits": [
      "Customized pricing plans",
      "Flexible scaling based on your cluster size",
      "Professional technical support",
      "Continuous feature updates and upgrades"
    ],
    "form": {
      "title": "Request Pricing Information",
      "name": "Name",
      "email": "Email",
      "company": "Company",
      "jobTitle": "Job Title",
      "nodeCount": "Number of Nodes",
      "gpuCount": "Number of GPUs",
      "message": "Message (Optional)",
      "submitButton": "Request Pricing",
      "submitting": "Submitting...",
      "submitSuccess": "Thank you! Your request has been submitted. Our team will contact you shortly with pricing information.",
      "submitError": "An error occurred while submitting your request. Please try again or contact us directly at info@dynamia.ai.",
      "nodeCountOptions": {
        "small": "Less than 10",
        "medium": "10-50",
        "large": "50-200",
        "enterprise": "More than 200"
      },
      "gpuCountOptions": {
        "small": "1-10",
        "medium": "10-50",
        "large": "50-200",
        "enterprise": "More than 200"
      }
    },
    "trustedBy": {
      "title": "Trusted by Industry Leaders",
      "description": "Leading organizations across various industries rely on Kantaloupe to manage their heterogeneous computing resources efficiently."
    }
  },
  "caseStudiesPage": {
    "title": "Case Studies",
    "subtitle": "See how teams scale AI, HPC, and inference workloads with HAMi and heterogeneous GPU scheduling.",
    "h1Prefix": "Case Study | ",
    "cardButton": "Read case study",
    "cta": {
      "title": "Ready to build your own success story?",
      "description": "Talk with our team about how HAMi can improve utilization, stability, and cost efficiency in your GPU infrastructure.",
      "button": "Request a Demo"
    }
  },
  "cases": {
    "telecomGpu": {
      "title": "Telecom Provider GPU Virtualization Case Study",
      "subtitle": "How a leading telecom provider leverages HAMi for GPU virtualization and heterogeneous device management",
      "overview": {
        "title": "Client Overview",
        "description": "A major telecommunications provider in Asia faced increasing demands for AI/ML capabilities in their network operations, customer service systems, and value-added services. With over 200 million subscribers and operations across multiple regions, the company needed an efficient way to manage their growing GPU infrastructure.",
        "keyPoints": [
          "Large-scale GPU Resource Pool",
          "Multi-tenant Isolation",
          "High Performance with Low Latency"
        ]
      },
      "challenge": {
        "title": "Challenges & Solutions",
        "subtitle": "Key Challenges",
        "points": [
          "Low GPU resource utilization (below 30%)",
          "Management complexity with isolated clusters",
          "Fluctuating workload demands",
          "High hardware acquisition costs"
        ]
      },
      "solution": {
        "subtitle": "HAMi Solution",
        "points": [
          "Fine-grained GPU sharing and virtualization",
          "Unified cluster management interface",
          "Intelligent workload scheduling",
          "Hardware-accelerated isolation"
        ]
      },
      "architecture": {
        "title": "Implementation Architecture",
        "description": "The HAMi solution provided a comprehensive virtualization layer that enabled unified management of heterogeneous GPU resources, with optimized scheduling and resource allocation."
      },
      "results": {
        "title": "Results & Benefits",
        "items": [
          {
            "title": "Resource Utilization",
            "value": "80%",
            "description": "Significant increase in GPU utilization"
          },
          {
            "title": "Cost Reduction",
            "value": "40%",
            "description": "Decrease in hardware and operational costs"
          },
          {
            "title": "Response Time",
            "value": "90%",
            "description": "Improvement in service deployment and response time"
          }
        ]
      },
      "cta": {
        "title": "Transform Your GPU Infrastructure",
        "description": "Discover how HAMi can help your organization achieve similar results with efficient GPU virtualization and heterogeneous computing management.",
        "button": "Contact Us"
      }
    },
    "sfTechnologyEffectiveGpu": {
      "title": "SF Technology Partners with HAMi to Enhance AI Efficiency and Significantly Reduce Costs with EffectiveGPU",
      "subtitle": "Discover how SF Technology built EffectiveGPU based on the open-source HAMi framework, deeply integrating heterogeneous computing virtualization and efficient scheduling capabilities to achieve production deployment in key scenarios like AI large model inference and voice recognition, significantly improving GPU utilization and realizing cost reduction and efficiency enhancement while promoting HAMi open-source ecosystem development.",
      "overview": {
        "title": "Company Overview",
        "description": "SF Technology is the technology arm of SF Express, one of China's leading logistics companies. As a technology-driven enterprise, SF Technology focuses on developing innovative solutions for logistics, AI, and cloud computing services.",
        "keyPoints": [
          "Leading logistics technology provider",
          "Extensive AI and machine learning applications",
          "Large-scale GPU infrastructure requirements",
          "Focus on cost optimization and efficiency"
        ]
      },
      "challenge": {
        "title": "Traditional GPU Management Challenges",
        "subtitle": "Core Pain Points",
        "description": "Traditional GPU usage patterns (such as whole-card exclusive allocation) led to GPUs being underutilized in inference and other light-load scenarios, resulting in serious resource waste.",
        "points": [
          "Low Resource Utilization: GPU average utilization remained below 30% for extended periods, with particularly pronounced idle computing power and memory issues in inference and testing scenarios.",
          "Coarse Scheduling Granularity: Lack of fine-grained resource partitioning and sharing capabilities, making it difficult to achieve multi-task concurrency and resource reuse.",
          "Heterogeneous Adaptation Difficulties: Mixed deployment of GPUs, NPUs, domestic AI chips and other multi-type devices posed ecosystem fragmentation and management complexity challenges for scheduling systems.",
          "Impact on ROI: These issues directly affected the deployment flexibility of AI services and the return on investment (ROI) of computing infrastructure."
        ]
      },
      "solution": {
        "title": "Breaking Through with EffectiveGPU Technology Practice",
        "subtitle": "Innovation Based on HAMi",
        "description": "Facing these challenges, SF Technology's team launched the EffectiveGPU technology solution based on the open-source heterogeneous computing scheduling framework HAMi, combined with their own business scenario requirements.",
        "objective": "The goal is to build an efficient, flexible, and unified GPU resource pooling and scheduling management system to solve problems of low resource utilization and management complexity.",
        "innovations": [
          {
            "title": "GPU Pooling and Virtualization",
            "description": "Integrated scattered GPU resources into a unified resource pool, enabling on-demand resource allocation through virtualization technology. This capability is based on and extends HAMi's virtualization foundation."
          },
          {
            "title": "Fine-grained Resource Partitioning",
            "description": "Supports precise partitioning by core utilization (computing power) and memory capacity, allowing a single GPU card to serve multiple applications with different requirements simultaneously, breaking the limitation of whole-card exclusive allocation. This benefits from HAMi's flexible partitioning mechanism."
          },
          {
            "title": "Elastic Resource Overcommitment",
            "description": "Introduced dual-dimension overcommitment technology for memory and computing power (up to 200% memory overcommitment ratio), combined with priority scheduling to further exploit GPU potential while ensuring QoS for high-priority tasks."
          },
          {
            "title": "Unified Management and Scheduling",
            "description": "Provides unified scheduling interfaces, abstracting and shielding underlying hardware differences, supporting unified management and efficient scheduling of heterogeneous resources including domestic GPUs. This aligns with HAMi's vision of building a unified abstraction driver framework."
          }
        ]
      },
      "results": {
        "title": "Significant Results: Substantial Improvement in Resource Utilization and Cost Reduction",
        "description": "The solution has completed multi-scenario deployment on SF Technology's AI platform, achieving remarkable results.",
        "items": [
          {
            "title": "Large Model Inference Services",
            "value": "28 GPUs → 65 Services",
            "description": "Deployed 65 services using 28 GPU cards, saving 37 cards"
          },
          {
            "title": "Testing Service Cluster", 
            "value": "6 GPUs → 19 Services",
            "description": "Deployed 19 services using 6 test GPU cards, saving 13 cards"
          },
          {
            "title": "Voice Recognition Services",
            "value": "Real-time Guaranteed",
            "description": "Ensured real-time performance for critical tasks through priority scheduling and resource overcommitment"
          },
          {
            "title": "Domestic Computing Adaptation",
            "value": "Multi-vendor Support",
            "description": "Successfully adapted Ascend, Kunlun and other domestic AI chips with complete scheduling support, with compatibility partially benefiting from HAMi's heterogeneous compatibility design"
          },
          {
            "title": "Performance Impact",
            "value": "Only 0.5% Degradation",
            "description": "Minimum performance decrease of only 0.5% after adding pooling layer"
          }
        ]
      },
      "hamiIntegration": {
        "title": "Deep Integration with HAMi Ecosystem, Building Efficient Computing Infrastructure",
        "description": "The successful practice of EffectiveGPU technology is inseparable from deep integration with the open-source heterogeneous computing scheduling framework HAMi.",
        "points": [
          "EffectiveGPU's technical architecture deeply integrates HAMi's core capabilities in heterogeneous computing virtualization, multi-heterogeneous GPU efficient scheduling, unified management, and observability.",
          "Particularly in domestic GPU management and scheduling, unified abstraction driver framework and cross-architecture scheduling models were built through HAMi ecosystem integration, achieving good adaptation and efficient utilization of domestic AI computing platforms including Huawei Ascend and Baidu Kunlun.",
          "EffectiveGPU solution adopts HAMi ecosystem-compatible designs in virtualization interfaces, scheduling interfaces, and heterogeneous GPU compatibility, ensuring smooth technology integration and seamless application operation."
        ]
      },
      "validation": {
        "title": "Validating HAMi Value, Promoting Open-Source Heterogeneous Computing Scheduling Maturity",
        "description": "SF Technology's successful EffectiveGPU deployment is another powerful validation of HAMi's technical concepts and engineering value.",
        "points": [
          "Proves that HAMi's key capabilities in flexible and reliable virtualization, efficient scheduling, unified management, and observability can fully support large enterprise actual needs in complex production environments.",
          "As a CNCF Sandbox & CNAI landscape project, this practice has accumulated valuable experience for HAMi's further promotion of industry standardization construction and scenario implementation."
        ]
      },
      "quote": {
        "text": "Through close collaboration with the HAMi open-source community and secondary innovation based on its framework, EffectiveGPU has helped us significantly improve GPU resource efficiency and reduce operational costs. This is an exemplary case of win-win cooperation between open-source collaboration and enterprise practice.",
        "author": "SF Technology AI Platform Lead",
        "position": "AI Platform Lead, SF Technology"
      },
      "conclusion": {
        "title": "Future Outlook",
        "description": "SF Technology has effectively solved the core pain points of GPU resource management through the EffectiveGPU solution based on HAMi, achieving cost reduction and efficiency enhancement. Looking forward, the HAMi ecosystem will attract more industry users to jointly promote the prosperity of heterogeneous computing ecosystems."
      },
      "companyCard": {
        "name": "SF Technology",
        "description": "Leading logistics technology provider in China"
      },
      "cta": {
        "exploreHami": "Explore HAMi",
        "contactUs": "Contact Us"
      }
    },
    "prepEduHami": {
      "title": "PREP EDU × HAMi | Intelligent Heterogeneous GPU Scheduling for Enhanced AI Service Efficiency",
      "subtitle": "PREP EDU deployed the HAMi framework to add fine-grained vGPU sharing and GPU aware scheduling to its production Kubernetes (RKE2) clusters. This addressed concrete problems: GPU utilization below 20%, frequent out-of-memory crashes from shared cards, and the operational overhead of manually scheduling workloads across mixed GPU hardware. The result was a measurable increase in inference task density and platform stability.",
      "overview": {
        "title": "Company Overview",
        "description": "PREP EDU is one of Southeast Asia’s fastest-growing edtech companies. It offers AI-powered language learning and test preparation services across Southeast Asia and relies on stable, large-scale, low-latency inference, making the efficiency of its underlying GPU infrastructure a primary cost and performance driver.",
        "keyPoints": [
          "A leading provider of AI-driven cross-border test-prep services",
          "Dedicated to implementing personalized AI-based learning scenarios and optimizing learning experiences",
          "Committed to addressing infrastructure scaling challenges in AI teaching environments",
          "Promotes open-source technologies in hands-on education and research environments"
        ]
      },
      "challenge": {
        "title": "Rising Complexity in Heterogeneous GPU Scheduling",
        "description": "As PREP EDU’s large-scale AI inference workloads expanded, traditional GPU usage models could no longer support the rapidly growing service demands. In a mixed-GPU environment (RTX 4070 / RTX 4090), resource waste, unbalanced scheduling, and compatibility issues became critical.",
        "points": [
          "Low GPU Utilization: Allocating GPUs exclusively as full cards prevented inference workloads from making effective use of available resources. Average utilization often remained as low as 10–20%, leaving both compute capacity and memory significantly underused.",
          "Frequent Resource Conflicts: Without proper isolation and scheduling mechanisms, competing workloads frequently triggered memory contention, pushing GPU memory usage to 90–95%. This led to application crashes, interrupted inference processes, and ultimately impacted overall service stability.",
          "Challenges in Heterogeneous Scheduling: In mixed-GPU environments combining RTX 4070 and 4090 models, different projects often required specific GPU types. Lacking a unified allocation and selection mechanism, resource dispatching became complex and error-prone.",
          "High Compatibility Barriers: Any new solution needed to remain fully compatible with existing components such as RKE2, GPU Operator, and containerd. Non-transparent or intrusive approaches risked increasing operational overhead or disrupting existing production workflows."
        ]
      },
      "solution": {
        "title": "Solution: Implementing Efficient GPU Orchestration with HAMi",
        "description": "To address long-standing issues such as heterogeneous scheduling complexity, low GPU utilization, and insufficient resource isolation, PREP EDU adopted HAMi to build a lightweight and non-intrusive GPU virtualization and orchestration solution within its existing RKE2 + GPU Operator + multi-model NVIDIA GPU environment. ",
        "objective": "Through this integration, PREP EDU significantly reduced resource waste and conflict frequency while ensuring the platform can scale smoothly with growing AI inference workloads.",
        "innovations": [
          {
            "title": "Virtualization & GPU Partitioning",
            "description": "Workloads received resource limits based on NLP token lengths and service needs, enabling precise vGPU allocation."
          },
          {
            "title": "Heterogeneous GPU Management",
            "description": "With HAMi, workloads can be scheduled by GPU type (e.g., run specific services only on RTX 4070 or 4090), using annotations to ensure compatibility and performance."
          },
          {
            "title": "Seamless Application Integration",
            "description": "Transparent device virtualization allows GPU sharing and isolation without modifying existing applications."
          },
          {
            "title": "GPU-Specific Assignment",
            "description": "Tasks can be allocated by GPU UUID, enabling multiple processes to run on a single 24GB RTX 4090 in a controlled manner."
          },
          {
            "title": "Full Compatibility",
            "description": "HAMi and NVIDIA GPU Operator coexist smoothly, both running on containerd. Combined with Prometheus monitoring, the system integrates seamlessly with RKE2 and containerd."
          }
        ]
      },
      "results": {
        "title": "Results: Major Increases in  GPU Utilization &  Inference Stability",
        "description": "The solution has been fully validated in PREP EDU’s production-scale inference platform. After adopting HAMi, PREP EDU successfully decoupled and automatically organized its GPU resources:",
        "items": [
          {
            "title": "Production Environment Usage",
            "value": "1+ Year",
            "description": "1+ years of stable production usage"
          },
          {
            "title": "GPU Infrastructure Optimization",
            "value": "90%",
            "description": "90% of GPU infrastructure optimized through HAMi"
          },
          {
            "title": "Reduce O&M Pain Points",
            "value": "50%",
            "description": "50% reduction in GPU-related operational incidents"
          }
        ]
      },
      "hamiIntegration": {
        "title": "Deep Integration with the HAMi Ecosystem to Build a High-Performance Inference Foundation",
        "description": "PREP EDU's progress in GPU virtualization and intelligent scheduling is built on its deep integration with the HAMi ecosystem.",
        "points": [
          "By integrating HAMi's device virtualization, fine-grained vGPU partitioning, heterogeneous scheduling, and built-in observability, PREP EDU is able to unify and efficiently share multiple GPU models—without any modifications to existing applications.",
          "By adopting HAMi's transparent virtualization, annotation-based scheduling, and UUID-level binding, PREP EDU achieves consistent scheduling across RTX 4070 and 4090 GPUs, allowing tasks to detect GPU types, allocate resources on demand, and run multiple instances concurrently. HAMi's seamless compatibility with GPU Operator, RKE2, and containerd also ensures that new nodes automatically join the unified resource pool.",
          "By validating HAMi in real production workflows—including Docker-based self-hosting, automated node onboarding, and joint optimization with GPU Operator—PREP EDU extends HAMi's applicability and demonstrates its flexibility and engineering maturity at scale."
        ]
      },
      "conclusion": {
        "title": "Future Outlook",
        "description": "By adopting HAMi-powered GPU virtualization and intelligent scheduling, PREP EDU has solved the core challenges of managing heterogeneous GPUs at scale, significantly improving inference efficiency and resource utilization. Looking forward, the HAMi ecosystem is poised to attract more industry adopters and further accelerate the growth of heterogeneous computing infrastructures."
      },
      "companyCard": {
        "name": "PREP EDU",
        "description": "Fast-growing EdTech company in Southeast Asia"
      },
      "cta": {
        "exploreHami": "Explore HAMi",
        "contactUs": "Contact Us"
      }
    },
    "keHoldings": {
      "title": "Ke Holdings Scales ML Infrastructure with GPU Virtualization using Kubernetes and HAMi",
      "subtitle": "Discover how Ke Holdings built AIStudio based on HAMi and Kubernetes, achieving nearly 3x GPU utilization improvement (13% → 37%) while supporting 10,000+ pods and 10M+ daily requests across hybrid cloud environments.",
      "stats": [
        { "value": "3x", "label": "GPU utilization improvement" },
        { "value": "10,000+", "label": "pods running simultaneously" },
        { "value": "10M+", "label": "daily requests processed" }
      ],
      "overview": {
        "title": "Company Overview",
        "description": "Ke Holdings Inc. is an integrated online and offline platform for housing transactions and related services based in China. The centralized infrastructure team operates a shared machine learning platform used across all business units, providing end-to-end compute services for model development, training, and large-scale inference.",
        "keyPoints": [
          "Leading housing transaction platform in China",
          "Centralized machine learning platform for all business units",
          "End-to-end compute services for AI workloads",
          "Massive GPU infrastructure across hybrid clouds"
        ]
      },
      "challenge": {
        "title": "Challenges in Scaling ML Infrastructure",
        "description": "As machine learning initiatives scaled, the infrastructure team faced significant challenges in GPU resource management across a complex hybrid-cloud environment.",
        "points": [
          {
            "title": "Scale and Complexity",
            "description": "5 clusters across public and private clouds, thousands of GPU cards including diverse models (H200, H20, V100, 4090, H100, A100)"
          },
          {
            "title": "Hybrid-cloud Environment",
            "description": "Managing GPU resources across public cloud (Volcano Engine, Tencent Cloud, Ali Cloud) and private cloud with ~1,000 NVIDIA GPUs"
          },
          {
            "title": "Diverse Workload Requirements",
            "description": "Large-scale model training requiring full GPU access vs. small model inference needing minimal GPU memory (1-2GB)"
          },
          {
            "title": "Low GPU Utilization",
            "description": "Only 13% initial utilization rate due to multi-cloud complexity and diverse workload requirements"
          }
        ]
      },
      "solution": {
        "title": "AIStudio Platform Built on Kubernetes and HAMi",
        "description": "Using CNCF projects HAMi and Kubernetes as foundation, Ke Holdings designed and implemented AIStudio, a smart computing platform serving as the basis for the organization's machine learning infrastructure.",
        "platform": "Leveraging Kubernetes and HAMi for GPU virtualization, AIStudio provides a unified platform bridging upper-layer SaaS services with underlying compute resources.",
        "features": [
          {
            "title": "Multi-scenario Support",
            "description": "Simultaneously supports inference services, A/B testing tasks, and training tasks on same infrastructure"
          },
          {
            "title": "Advanced Optimization",
            "description": "Acceleration capabilities for inference frameworks, datasets, images, checkpoints, and models with fault tolerance"
          },
          {
            "title": "Multi-framework Support",
            "description": "PyTorch, DeepSpeed, Megatron, VLLM, RLHF, and SGLang"
          },
          {
            "title": "AI Asset Management",
            "description": "Centralized management of resource pools, model repositories, image repositories, queues, CubeFS volumes, and monitoring"
          }
        ],
        "architecture": {
          "title": "Dual-Cluster Architecture for Different Workloads",
          "gpuCluster": {
            "title": "GPU Clusters",
            "description": "Managed by native NVIDIA device plugin for training workloads requiring complete GPU resources:",
            "items": [
              "Native NVIDIA device plugin",
              "High-performance GPUs (H200, H100)",
              "Dedicated for LLM training",
              "Full GPU resource allocation"
            ]
          },
          "vgpuCluster": {
            "title": "vGPU Clusters",
            "description": "Managed by HAMi for GPU memory virtualization for small model inference:",
            "items": [
              "HAMi GPU memory virtualization",
              "GPUs (H20, V100, A100, 4090)",
              "Fine-grained allocation (1-2GB)",
              "Small model inference"
            ]
          }
        }
      },
      "results": {
        "title": "Significant Results: 3x GPU Utilization Improvement",
        "description": "By leveraging open-source technologies including HAMi and Kubernetes, AIStudio has achieved remarkable results at massive scale.",
        "items": [
          { "title": "GPU Utilization", "value": "13% → 37%", "description": "Nearly 3x improvement" },
          { "title": "Platform Scale", "value": "10,000+ pods", "description": "Running simultaneously" },
          { "title": "Daily Requests", "value": "10M+", "description": "Processed per day" },
          { "title": "Cluster Coverage", "value": "5 clusters", "description": "Public and private cloud" },
          { "title": "Zero Downtime", "value": "100%", "description": "During transition and operation" },
          { "title": "Workload Types", "value": "Unified", "description": "Training and inference on same platform" }
        ]
      },
      "hamiIntegration": {
        "title": "HAMi Enables GPU Multiplexing and Heterogeneous Scheduling",
        "description": "The successful integration of HAMi demonstrates how open-source technologies enable organizations to achieve remarkable infrastructure efficiency.",
        "points": [
          "Kubernetes serves as the foundation for stable operations with robust scheduling and management capabilities",
          "HAMi enables GPU multiplexing and heterogeneous scheduling optimization, increasing cluster GPU utilization by nearly 3x",
          "Dual-cluster approach separates workloads based on resource requirements for optimal efficiency",
          "Seamless integration between public and private cloud environments enables unified platform management"
        ]
      },
      "futurePlans": {
        "title": "Future Innovation Plans",
        "description": "Ke Holdings' infrastructure team continues to innovate and expand their platform on top of HAMi and Kubernetes.",
        "plans": [
          "Adopting heterogeneous devices: Plans to incorporate Huawei Ascend and other non-NVIDIA accelerators",
          "Cloud expansion: Integration with Alibaba Cloud to complement existing Volcano Engine and Tencent Cloud deployments",
          "Advanced scheduling policies: Network topology-awareness, card type specification, and UUID-based allocation"
        ]
      },
      "companyCard": {
        "name": "Ke Holdings",
        "description": "Leading housing transaction platform in China"
      },
      "cta": {
        "exploreHami": "Explore HAMi",
        "contactUs": "Contact Us"
      },
      "conclusion": {
        "title": "Open-Source Success Story",
        "description": "Ke Holdings has successfully demonstrated how leveraging HAMi and Kubernetes can dramatically improve GPU utilization while supporting massive-scale AI workloads. The AIStudio platform serves as a model for organizations seeking to optimize their machine learning infrastructure."
      }
    },
    "daoCloud": {
      "title": "Building Flexible GPU Clouds with HAMi at DaoCloud",
      "subtitle": "Discover how DaoCloud operates two major cloud native platforms for AI workloads—D.run Compute Cloud and DaoCloud Enterprise—using HAMi to achieve >80% GPU utilization across 10,000+ GPUs spanning 10+ data centers.",
      "stats": [
        { "value": "10,000+", "label": "GPUs across platforms" },
        { "value": ">80%", "label": "average GPU utilization" },
        { "value": "20-30%", "label": "reduction in operating costs" }
      ],
      "overview": {
        "title": "Company Overview",
        "description": "DaoCloud operates two major cloud native platforms for AI workloads. D.run Compute Cloud is a public GPU cloud serving individual developers and small teams, while DaoCloud Enterprise (DCE) is a private Kubernetes platform for enterprise customers running both training and inference.",
        "keyPoints": [
          "Two major cloud native platforms",
          "D.run Compute Cloud for public GPU cloud",
          "DaoCloud Enterprise (DCE) for private K8s",
          "10+ data centers across China and Hong Kong"
        ]
      },
      "challenge": {
        "title": "Challenges in GPU Resource Management",
        "description": "As GPU demand grew rapidly across both platforms, several challenges emerged that required a flexible GPU virtualization solution.",
        "points": [
          {
            "title": "Whole-card Allocation",
            "description": "Many inference and lightweight workloads used only a fraction of GPU resources, leaving significant portions of compute and memory underutilized and limiting how DaoCloud could package GPU SKUs."
          },
          {
            "title": "Heterogeneous Hardware",
            "description": "DaoCloud needed to support mainstream NVIDIA GPUs while also integrating domestic accelerators from multiple vendors. Proprietary vGPU solutions increased licensing costs."
          },
          {
            "title": "Multi-tenant Governance",
            "description": "On DCE, enterprise customers wanted shared GPU pools with department-level quotas, queue-based resource allocation, and clear isolation across teams."
          },
          {
            "title": "Cloud Native Alignment",
            "description": "DaoCloud's core strategy revolves around Kubernetes and open-source technologies. Any GPU sharing solution had to stay fully cloud native, vendor-agnostic, and compatible with existing CNCF tooling."
          }
        ]
      },
      "solution": {
        "title": "HAMi as the Unified GPU Layer",
        "description": "DaoCloud adopted HAMi, a CNCF Sandbox project, for heterogeneous AI computing virtualization, as the unified GPU layer across both D.run and DCE. HAMi provides device virtualization, vGPU partitioning, and scheduling for heterogeneous accelerators in Kubernetes clusters.",
        "drun": {
          "title": "D.run Compute Cloud: vGPU SKUs for Public GPU Users",
          "description": "On D.run, DaoCloud integrated HAMi into each regional Kubernetes cluster to enable fine-grained GPU sharing and higher utilization.",
          "features": [
            {
              "title": "vGPU Slicing",
              "description": "Physical GPUs partitioned into multiple vGPU slices with defined compute and memory. Lightweight inference jobs can run on fractional GPUs."
            },
            {
              "title": "SKU-based Marketplace",
              "description": "vGPU slices are exposed as standardized SKUs in a central marketplace. Users select GPU SKUs based on workload size."
            },
            {
              "title": "Multi-region Deployment",
              "description": "HAMi powers 7 active D.run regions across Mainland China and Hong Kong, covering over 10 data centers."
            },
            {
              "title": "Domestic Accelerator Support",
              "description": "DaoCloud extended HAMi to support domestic GPU vendors, ensuring consistent management under a unified abstraction layer."
            }
          ]
        },
        "dce": {
          "title": "DaoCloud Enterprise (DCE): Shared GPU Pool for Enterprise",
          "description": "On DCE, DaoCloud built a centralized GPU resource pool using HAMi, unifying GPU capacity for multiple enterprise tenants.",
          "features": [
            {
              "title": "Unified GPU Pool",
              "description": "Enterprise users contribute and consume GPUs from a central pool that serves both training and inference workloads."
            },
            {
              "title": "Quotas & RBAC",
              "description": "HAMi's vGPU resources are integrated with DaoCloud's existing quota and role-based access systems."
            },
            {
              "title": "Simplified Experience",
              "description": "Algorithm engineers request GPU resources through the platform without worrying about underlying hardware differences."
            }
          ]
        }
      },
      "opensource": {
        "title": "Co-developing HAMi with the Community",
        "description": "DaoCloud has been one of HAMi's earliest and most active contributors.",
        "points": [
          "Contributed real-world insights from D.run and DCE back to the open-source community",
          "Collaborated upstream to improve GPU over-subscription mechanisms, node configuration management, and heterogeneous hardware handling",
          "Helped maintain documentation and deployment guides to support production adoption by other cloud providers"
        ]
      },
      "results": {
        "title": "Significant Results: Cost Reduction and Improved Efficiency",
        "description": "By integrating HAMi, DaoCloud consolidated previously fragmented GPU resources into a more unified, efficient, and scalable GPU layer across both public and private clouds.",
        "items": [
          { "title": "GPU Utilization", "value": ">80%", "description": "Average utilization per card after HAMi deployment" },
          { "title": "Cost Reduction", "value": "20-30%", "description": "Reduction in GPU-related operating costs" },
          { "title": "Unified Abstraction", "value": "Single Layer", "description": "Across NVIDIA and domestic GPUs" },
          { "title": "Deployment Scale", "value": "10,000+ GPUs", "description": "Across 10+ data centers" },
          { "title": "Multi-region", "value": "7 Regions", "description": "Active D.run regions across China" },
          { "title": "Open Collaboration", "value": "Active", "description": "Contributing improvements upstream" }
        ]
      },
      "quote": {
        "text": "HAMi is more than compatible with DaoCloud's business, it's something we've built together. As one of HAMi's earliest contributors, we've witnessed its evolution from inception to maturity. HAMi now runs across both D.run and DCE, and our real-world improvements continuously flow back to the community. HAMi and DaoCloud share the same open-source DNA, and we'll continue contributing to HAMi to bring true vGPU technology to the world.",
        "author": "Captain, AI/LLM Infra Product Lead, DaoCloud"
      },
      "companyCard": {
        "name": "DaoCloud",
        "description": "Cloud native platform provider for AI workloads"
      },
      "cta": {
        "exploreHami": "Explore HAMi",
        "contactUs": "Contact Us"
      },
      "conclusion": {
        "title": "Open-Source Partnership Success",
        "description": "DaoCloud has successfully integrated HAMi across both its public and private GPU cloud platforms, achieving dramatic improvements in utilization and cost efficiency while contributing back to the open-source community."
      }
    }
  },
  "privacyPolicy": {
    "title": "Privacy Policy",
    "lastUpdated": "Last Updated: March 03, 2025",
    "intro": "DYNAMIA INTELLIGENCE PTE. LTD. (referred to as \"us\", \"we\", or \"our\" hereinafter) is the operator of the website https://dynamia.ai/ (the \"Site\"). This page details our policies regarding the collection, utilization, and disclosure of Personal Information we obtain from users of the Site.",
    "consent": "We utilize your Personal Information solely for the purposes of providing and enhancing the Site. By using the Site, you consent to the collection and use of information in line with this policy.",
    "sections": [
      {
        "title": "Information Collection and Use",
        "content": "When using our Site, we may request that you supply certain personally identifiable information that can be employed to contact or identify you. Personally identifiable information may encompass, but is not restricted to, your name and email address."
      },
      {
        "title": "Log Data",
        "content": "Similar to numerous site operators, we gather information that your browser transmits each time you visit our Site (referred to as \"Log Data\"). This Log Data may include details such as your computer's Internet Protocol (IP) address, browser type, browser version, the pages of our Site you visit, the time and date of your visit, the duration spent on those pages, and other relevant statistics. Additionally, we may employ third - party services like Google Analytics to collect, monitor, and analyze this data."
      },
      {
        "title": "Communications",
        "content": "We may utilize your email address to send you newsletters, marketing or promotional materials, and other information that aids you in better understanding and using our products and services. You can always unsubscribe from such communications by clicking the unsubscribe link within the email or by updating your preferences on the Site."
      },
      {
        "title": "Cookies",
        "content": "Cookies are small data files that may contain an anonymous unique identifier. They are sent from a website to your browser and stored on your computer's hard drive. Like many sites, we use \"cookies\" to collect information. You can configure your browser to reject all cookies or to alert you when a cookie is being sent. However, if you choose not to accept cookies, you may encounter difficulties accessing and using some parts of our Site."
      },
      {
        "title": "Security",
        "content": "The security of your Personal Information is of great importance to us. Nevertheless, it's crucial to note that no method of transmission over the Internet or electronic storage is completely secure. Although we endeavor to use commercially reasonable means to safeguard your Personal Information, we cannot ensure its absolute security."
      },
      {
        "title": "Changes to This Privacy Policy",
        "content": "This Privacy Policy became effective on March 03, 2025, and will remain in force, except for any future changes to its provisions. Such changes will take effect immediately upon being posted on this page. We reserve the right to update or modify our Privacy Policy at any time. You are advised to review this Privacy Policy periodically. Continuing to use the Service after we post any modifications to the Privacy Policy on this page will signify your recognition of the changes and your consent to comply with and be bound by the revised Privacy Policy. If we make any significant changes to this Privacy Policy, we will notify you either via the email address you provided us or by placing a conspicuous notice on our website."
      },
      {
        "title": "Contact Us",
        "content": "If you have any queries regarding this Privacy Policy, please contact us at info@dynamia.ai."
      }
    ]
  },
  "cookiesPolicy": {
    "title": "Cookies Policy",
    "lastUpdated": "Last Updated: March 03, 2025",
    "intro": "When you visit or access our websites or applications (collectively referred to as \"Services\"), or interact with our content, we utilize (and authorize third parties to use) web beacons, cookies, pixel tags, scripts, tags, APIs, and other technologies (collectively known as \"Tracking Technologies\").\n\nThese Tracking Technologies enable us to automatically gather information about you, your online behavior, and your device (such as your computer or mobile device). This information is used to enhance your navigation on our Services, improve their performance, and customize your experience. We also use this data to collect statistics on Service usage, conduct analytics, deliver content tailored to your interests, and manage services for our users, advertisers, publishers, customers, and partners.\n\nMoreover, we permit third parties to collect information about you through Tracking Technologies.",
    "sections": [
      {
        "title": "What are cookies?",
        "content": "Cookies are small text files consisting solely of letters and numbers. A web server places them on your computer or mobile device when you visit a webpage. Cookies can enhance the user - friendliness of our Services, for instance, by remembering your language preferences and settings. For more details about cookies, visit www.allaboutcookies.org.\n\nCookies are extensively used to make websites function efficiently. They allow you to navigate between pages smoothly, remember your preferences, and make your interaction with the Services more seamless and efficient. Cookies also help ensure that the online advertisements you see are relevant to you and your interests."
      },
      {
        "title": "Storing Tracking Technologies",
        "content": "We store Tracking Technologies when you visit or access our Services (e.g., when browsing our websites); these are termed \"First Party Tracking Technologies\". Additionally, third parties such as our analytics service providers, business partners, and advertisers store Tracking Technologies while running content on our Services; these are called \"Third Party Tracking Technologies\".\n\nBoth types of Tracking Technologies may be stored either during your visit to our Services or for subsequent repeat visits."
      },
      {
        "title": "What types of Tracking Technologies do we use?",
        "content": "There are five primary types of Tracking Technologies:\n\n1. Strictly necessary Tracking Technologies: These are essential for you to log in, navigate, and use the features of our Services or receive a service you've requested (like your username). We don't require your consent to use these. They can be used for security and integrity purposes, such as detecting policy violations and enabling support or security features.\n2. Functionality Tracking Technologies: These enable our Services to remember your choices (e.g., your language) and offer enhanced, personalized features. For example, they're used for authentication (to recall when you're logged - in) and to support other Service features.\n3. Performance Tracking Technologies: These collect information about your online activity (e.g., the length of your visit to our Services), including behavioral data and content engagement metrics. They're used for analytics, research, and generating statistics (based on aggregated information).\n4. Marketing or Advertising Tracking Technologies: These are used to deliver customized offers and ads to you based on your inferred interests and to conduct email marketing campaigns. They can limit the number of times you see an ad and help measure the effectiveness of advertising campaigns. Usually placed by our advertisers (e.g., advertising networks), they provide insights to advertisers about those who view and interact with their ads, visit their websites, or use their apps.\n5. Social media Tracking Technologies: Our website incorporates social media features like Facebook's \"Like\" or \"Share\" buttons. These features are either hosted by a third party or directly on our Services. Your interactions with these features are subject to the privacy statement of the company providing them."
      },
      {
        "title": "Your choices regarding cookies",
        "content": "If you would like to restrict or block cookies, you can do this through your browser settings. The Help function within your browser should tell you how. Alternatively, you can visit www.aboutcookies.org which contains comprehensive information on how to restrict or block cookies on a wide variety of browsers. You can also manage your cookie preferences through our cookie consent banner that appears when you first visit our Site."
      },
      {
        "title": "Changes to our cookie policy",
        "content": "We may update our Cookie Policy from time to time. Changes to our Cookie Policy will be updated on this page. We encourage users to frequently check this page for any changes. The date of the last update to our Cookie Policy is indicated at the top of the policy."
      },
      {
        "title": "Contact us",
        "content": "If you have any questions about our use of cookies or other technologies, please email us at info@dynamia.ai."
      }
    ]
  },
  "common": {
    "readMore": "Read More",
    "viewAll": "View All",
    "useCompanyEmail": "Please use a company email to submit the form.",
    "submitting": "Submitting..."
  }
} 
