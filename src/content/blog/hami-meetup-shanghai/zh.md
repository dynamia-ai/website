---
title: 不卷算力卷效率｜HAMi Meetup 上海站技术笔记全回顾！
date: '2025-12-16'
excerpt: >-
  本次 HAMi Meetup 于 11 月 30 日在上海成功举办，围绕「不卷算力卷效率」主题，聚焦 GPU 虚拟化与异构算力调度实践。来自 CNCF
  及多家企业的专家分享了 HAMi 在调度可观测性、GPU 共享、国产算力适配与云原生 AI 基建方面的最新进展与落地经验。
author: 密瓜智能
tags:
  - HAMi
  - 异构算力调度
  - GPU 虚拟化
  - 云原生 AI
  - Kubernetes
  - 国产算力
  - AI 训练与推理优化
  - 开源生态
  - HAMi Meetup
category: Community & Events
coverImage: /images/blog/hami-meetup-shanghai/cover.png
language: zh
---

![HAMi Meetup 上海站](/images/blog/hami-meetup-shanghai/cover.png)

11 月 30 日，首场 HAMi Meetup 在上海模速空间圆满结束！本次活动以"不卷算力卷效率"为主题，近百位 AI 开发者、运维工程师、企业 IT 架构师齐聚现场，线上 700 多名观众通过直播间实时围观，共同聚焦异构算力调度的核心命题。CNCF、密瓜智能、蔚来、沐曦股份、DaoCloud、星环科技的实战派专家轮番登场，围绕异构芯片适配、资源利用率提升、AI 训练优化、国产算力落地的核心痛点，不仅拆解了可直接复用的技术方案，更毫无保留分享了落地过程中的协同经验与避坑指南，干货密度拉满！

![HAMi Meetup 上海站活动现场](/images/blog/hami-meetup-shanghai/image1.png)

## 重磅开场

![CNCF Keith Chan 开场演讲](/images/blog/hami-meetup-shanghai/image2.png)

Linux 基金会副总裁，CNCF 亚太地区中国主席 Keith Chan 进行了开场演讲。指出 HAMi 在 GPU 虚拟化方向的突破，使其成为国内外企业共同推进云原生 AI 基础建设的重要力量，也推动项目进入 CNCF 孵化阶段并持续加速成长。随着算力需求爆发式增长，企业不仅关注"拥有多少 GPU"，更关注"如何高效使用 GPU"。在生成式 AI 兴起后，越来越多的大模型应用开始全面迁移至云原生环境，70%–80% 的推理与训练工作负载已经运行在 Kubernetes 上。并指出在全球调研中，超过 80% 的企业认为"开源是 AI 成熟的关键驱动力"，而多云部署成为大模型时代的行业共识。基于此，CNCF 正与全球厂商联合推动 Certified AI Platform for Kubernetes 标准化计划，通过 100+ 项 Conformance Test 覆盖加速器、存储、网络、调度、可观测性与安全等核心能力，保障 AI 工作负载在多云环境下的兼容性、可移植性与可观测性。随着云原生在 AI、边缘、量子计算与分布式系统中的全面铺开，大模型时代的基础设施竞争将从"堆卡"转向"效率战"。HAMi 在 GPU 虚拟化与异构调度上的持续推进，将为全球 AI 基建提供关键能力，也将与 CNCF 一起推动下一代云原生 AI 生态的繁荣。

## 五大议题全回顾

### 《HAMi 2.7.0 版本解读与 2.8.0 版本 Roadmap：异构芯片全适配与调度能力跨越式升级》

![HAMi 核心 maintainer 李孟轩演讲](/images/blog/hami-meetup-shanghai/image3.png)

HAMi 核心 maintainer 李孟轩在演讲中介绍了 HAMi 从 2.7.0 到 2.8.0 的能力演进：一方面，在"可用性"上，2.7.0 新增调度原因可视化与资源配额监控优化，用户不仅能一眼看出 Pod 为什么 Pending，还能更准确统计显存配额使用，解决虚拟化带来的 quota 失真问题；另一方面，在"生态与硬件"上，HAMi 已支持 9 家厂商 GPU，并扩展到昆仑芯 XPU、AWS Trainium / Inferentia 等异构设备，通过 Web UI 将这些能力以更友好的方式呈现。面向未来，2.8.0 版本重点优化调度性能与 Web UI 的异构设备支持，并规划通过 DRA 将原有 scheduler 与 device plugin 能力收敛到新的 DRA driver 中，在不改变用户使用习惯的前提下，提供更标准化的动态切分、硬隔离与多维资源指定能力，为后续大规模集群落地打下基础。

### 《MetaX sGPU on HAMi》

![沐曦股份郭磊分享 MetaX sGPU](/images/blog/hami-meetup-shanghai/image4.png)

沐曦股份的云原生基础架构专家 & MetaX sGPU 核心贡献者郭磊分享了 sGPU 在 HAMi 社区落地的全过程：sGPU 通过细粒度切分显存与算力（显存以 1MB 级粒度、算力以 1% 粒度配置），在 HAMi 调度器与 MetaX device plugin 的配合下，让 Pod 可以按需申请诸如"60% 算力 + 4GB 显存"的虚拟 GPU 资源。在调度层面，HAMi 原生提供节点级与 GPU 级 binpack/spread 策略，可灵活组合以兼顾资源利用率与业务隔离度；在 QoS 层面，sGPU 通过 best effort、fixed share、burst share 多种策略，支持从"完全抢占"到"弹性配额"的不同算力保障需求，并结合在线/离线混部策略，在资源紧张时自动暂停低优先级离线任务，为在线推理任务让路。配合拓扑感知调度，整卡任务还能优先获得链路更优的 GPU 组合。郭老师总结道：sGPU 与 HAMi 的深度整合，使沐曦能够在 K8S 体系内，以标准接口向上层业务输出"可共享、可调度、可精细控制"的 GPU 服务。

### 《基于 vGPU 的 mlperf》

![蔚来李鹏分享基于 vGPU 的 mlperf](/images/blog/hami-meetup-shanghai/image5.png)

在实际业务场景中，虚拟化不是"是否使用"的问题，而是"用哪一种副作用最小"的选择题。来自蔚来的云端工程部训练加速负责人李鹏围绕"基于 HAMi vGPU 的训练性能优化框架"展开分享，重点介绍了在虚拟化环境下实现高精度性能诊断的完整路径。李鹏指出，HAMi 基于 perf 的虚拟化机制为性能分析提供了天然入口：通过拦截 CUDA/cuBLAS/NVML 等 GPU 核心库，无需改动业务代码即可捕获 GPU 全链路函数调用。这种无侵入式数据采集使得未来团队能够全面记录显卡执行逻辑、kernel 调度、显存使用与通信行为，并补充以外部 perf 采样构建 CPU/GPU 双侧时间线。通过时间轴融合，可精确还原任务在任意时间点的执行状态，识别低并行度、通信阻塞或异常 API 参数等关键瓶颈。在产品化层面，该框架实现了"底层瓶颈定位 + 上层代码溯源"的闭环诊断：既能量化算力与显存利用效率，也能跨进程关联具体业务模块。该体系已成功应用于蔚来自动驾驶训练场景，为大模型推理链路优化、多模态任务提速提供参考，显著减少了 GPU 资源浪费。

### 《HAMi：让开源 GPU 调度赋能 d.run 的 AI 开发者》

![DaoCloud 卢传佳分享 d.run 平台](/images/blog/hami-meetup-shanghai/image6.png)

来自「DaoCloud」道客的产品负责人卢传佳分享了 d.run 智算调度平台在 SaaS GPU 租赁场景 × HAMi 虚拟化能力上的深度实践。d.run 以云原生方式将各类 GPU 资源池化输出给用户，支持训练、推理、数据处理等形态；在算力云业务中，HAMi 作为底层虚拟化核心，为平台提供国产 GPU 统一接入、显存与算力的灵活切片能力。卢老师指出，企业自建场景普遍受限于"整卡"使用方式，利用率难以突破；而 SaaS 租赁模式虽然可依靠即时付费提升资源弹性，却极易受到供需波动影响，使 GPU 池出现"既碎片化又不稳定"的运营风险。HAMi 的动态切片与超配能力在此发挥关键价值——不仅显著降低碎片化，让单卡可输出更多 SKU（如 3G / 6G / 12G / 24G 等），还通过显存动态扩容避免因 OOM 触发容器重启，提升资源稳定性与库存精细化管理。目前，d.run 已基于 HAMi 构建 GPU 切片租赁平台，实现多集群池化管理、国产卡统一调度、租户级优先级与抢占策略，并在推理 MaaS、企业大模型部署与 C 端 GPU 云使用等多场景落地。卢老师表示，随着 HAMi DRA、Quota 改进等新能力成熟，算力云将进一步实现资源复用最大化，让 GPU 资产的商业价值被"切得更细、用得更满"。

### 《星环 x 国产算力生态：适配实践》

![星环科技侯雨希分享国产算力生态](/images/blog/hami-meetup-shanghai/image7.png)

来自星环科技的人工智能产品部 AI - 工具平台研发侯雨希围绕 LLMOps 平台在寒武纪、海光等国产加速器上的适配实践展开分享，重点介绍了如何借助 HAMi 统一管理多型号、多架构的国产 GPU，并结合 DRA 构建可扩展的算力抽象。侯老师介绍，星环 LLMOps 平台承担企业级大模型全生命周期管理任务，需同时满足多租户隔离、资源组管理、任务排队策略、自定义显存/算力切片模板与全链路可观测性。为此，HAMi 成为连接不同 AI 加速器的关键底座。在寒武纪适配中，团队解决了 sMLU 动态切片的粒度限制、多型号资源名隔离，以及硬编码显存单元导致的总量无法计算等问题，并通过节点 label、型号识别与资源对象数组配置，实现与昇腾/英伟达一致的多型号管理能力。在海光 DCU 场景中，团队进一步解决设备 ID 不唯一、exporter 无法正确暴露指标、Pod–设备映射错误等问题，通过驱动 SDK 获取硬件序列号、重建设备唯一 ID，并重写 exporter 逻辑以匹配哈密设备插件的记录方式，确保指标与调度一致性。面向未来，侯老师指出 DRA 将成为异构 GPU 的统一抽象方向，通过 DRA 新增的 API，以及可消耗的容量和可切分设备的特性，使算力切分完成从自定义到 Kubernetes 原生的升级。星环科技也将继续与 HAMi 社区合作，推动国产 AI 算力的统一调度与更标准化的算力建模体系。

## 交流分享，互动拉满

现场交流环节气氛热烈，参会者积极参与互动。围绕 GPU 虚拟化实践、模型推理效率优化、国产加速器适配经验等主题，提出了多角度、具备实操价值的问题，引发在场嘉宾与企业技术团队的深度讨论。现场席位持续满员，不少参会者在会后继续与讲者沟通技术细节、交换经验。整体交流充分体现了行业对算力效率与云原生 AI 基建的高关注度与实践探索意愿。

![HAMi Meetup 现场交流环节](/images/blog/hami-meetup-shanghai/image8.png)

## 结语

本次 HAMi Meetup 聚焦提升算力效率，不仅沉淀了多份企业级实战干货与行业协同共识，更让现场与线上的开发者们实现了深度思维碰撞 —— 从技术细节探讨到业务场景适配，从开源生态共建到国产算力创新，交流热度持续升温。未来，HAMi 社区将继续以开源力量为纽带，联动更多行业伙伴与开发者，深耕异构算力调度领域，让轻量化、高适配的算力提效方案惠及更多企业，共同推动数字经济高质量发展！

**期待下次 Meetup——12 月我们将和大家相约北京**，与大家继续聊芯聊序：聚焦异构算力虚拟化的利用率突破，深度拆解「算力资源闲置、异构硬件适配难导致利用率偏低」等核心痛点！

同时诚邀**社区用户和从业者踊跃投稿**：可以是你的异构算力虚拟化利用率优化经验，也可以是所在企业的落地案例，让优质实践在社区中流动，共探异构算力虚拟化提效的落地路径！
